{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-e632a2196fc9>, line 55)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-e632a2196fc9>\"\u001b[1;36m, line \u001b[1;32m55\u001b[0m\n\u001b[1;33m    vocab_list = [line.rstrip('\\n') for line in open(self.vocab_path,,encoding='utf-8')]\u001b[0m\n\u001b[1;37m                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import json\n",
    "import jieba\n",
    "\n",
    "\n",
    "\n",
    "class CHINESE(object):\n",
    "    def __init__(self,config):\n",
    "        super(CHINESE, self).__init__()\n",
    "        self.chinese_root = config.chinese_dataset_root\n",
    "        self.store_path = config.chinese_save_root\n",
    "        self.vocab_path = config.vocab_path\n",
    "        \n",
    "    \n",
    "    def extract_words(self):\n",
    "        word_list=[]\n",
    "        self.train_dir = os.path.join(self.chinese_root,'TRAIN')\n",
    "        WAV_files = sorted(glob.glob(self.train_dir+'/*.WAV'))\n",
    "        i=0\n",
    "        for audio_filepath in WAV_files:\n",
    "            print(i)\n",
    "            i+=1\n",
    "            #print(audio_filepath)\n",
    "            txt_file = audio_filepath[:-4]+'.txt'\n",
    "            lines = open(txt_file, 'r',encoding='utf-8') \n",
    "            for line in lines:\n",
    "                #print(line)\n",
    "                words = jieba.lcut(line)\n",
    "                for item in words:\n",
    "                    if item not in word_list:\n",
    "                        word_list.append(item)\n",
    " \n",
    "\n",
    "        #print(word_list)\n",
    "        #print(len(word_list))\n",
    "        return word_list\n",
    "    \n",
    "    def create_vocab(self):\n",
    "        word_list = self.extract_words()\n",
    "        full_chars_list = []\n",
    "        for word in word_list:\n",
    "            full_chars_list.append(word)\n",
    "        full_chars_list.append('<s>')\n",
    "        full_chars_list.append('</s>')\n",
    "        full_chars_list.append('_')\n",
    "        vocab = sorted(list(set(full_chars_list)))\n",
    "        fid_vocab = open(self.vocab_path,'w',encoding='utf-8')\n",
    "        for item in vocab:\n",
    "            fid_vocab.write(item+'\\n')\n",
    "        fid_vocab.close()\n",
    "    \n",
    "    def create_vocab_dict(self):\n",
    "        vocab_list = [line.rstrip('\\n') for line in open(self.vocab_path,,encoding='utf-8')]\n",
    "        i=0\n",
    "        vocab_dict={}\n",
    "        for item in vocab_list:\n",
    "            vocab_dict[item] = i\n",
    "            i+=1\n",
    "        return vocab_dict\n",
    "    \n",
    "        \n",
    "    def create_char_mapping(self,word_file,vocab_dict):\n",
    "        lines = open(word_file, 'r',encoding='utf-8')\n",
    "        for line in lines:\n",
    "            words = jieba.lcut(line)\n",
    "            char_mapped=[]\n",
    "            chars_list=[]\n",
    "            chars_list.append('<s>')\n",
    "            char_mapped.append(vocab_dict['<s>'])\n",
    "            for item in words:\n",
    "                word =item\n",
    "                char = item\n",
    "                try:\n",
    "                    phns = vocab_dict[char]\n",
    "                except:\n",
    "                    continue\n",
    "                char_mapped.append(phns)\n",
    "                chars_list.append(char)\n",
    "                char_mapped.append(vocab_dict['_'])\n",
    "                chars_list.append('_')\n",
    "            char_mapped.append(vocab_dict['</s>'])\n",
    "            chars_list.append('</s>')\n",
    "        #print(chars_list)\n",
    "        return char_mapped,chars_list\n",
    "    \n",
    "    \n",
    "            \n",
    "    def process_data_train(self):\n",
    "        if not os.path.exists(self.store_path):\n",
    "            os.makedirs(self.store_path)\n",
    "        self.train_store_path = os.path.join(self.store_path,'TRAIN')\n",
    "        if not os.path.exists(self.train_store_path):\n",
    "            os.makedirs(self.train_store_path)\n",
    "        vocab_dict = self.create_vocab_dict()\n",
    "        self.train_dir = os.path.join(self.chinese_root,'TRAIN')\n",
    "        WAV_files = sorted(glob.glob(self.train_dir+'/*.WAV'))\n",
    "        i = 1\n",
    "        for audio_filepath in WAV_files:\n",
    "            print(i)\n",
    "            i += 1\n",
    "            txt_file = audio_filepath[:-4]+'.txt'\n",
    "            char_mapped,chars_list = self.create_char_mapping(txt_file,vocab_dict)\n",
    "            json_write_filepath =self.train_store_path+'/'+audio_filepath.split('\\\\')[-1][:-4]+'.json'\n",
    "            data_frame = {}\n",
    "            data_frame['audio_filepath'] = audio_filepath.replace('\\\\','/').replace('..','.')\n",
    "            data_frame['char_map_seq'] = ' '.join([str(char_item) for char_item in char_mapped])\n",
    "            data_frame['chars'] = ' '.join([str(char_item) for char_item in chars_list])\n",
    "            #print(chars_list)\n",
    "            data_frame['char_seq_len']=len(char_mapped)\n",
    "            with open(json_write_filepath, 'w',encoding='utf-8') as fid:\n",
    "                json.dump(data_frame, fid,ensure_ascii=False,indent=4)\n",
    "                        \n",
    "        \n",
    "    def process_data_test(self):\n",
    "        if not os.path.exists(self.store_path):\n",
    "            os.makedirs(self.store_path)\n",
    "        self.test_store_path = os.path.join(self.store_path,'TEST')\n",
    "        if not os.path.exists(self.test_store_path):\n",
    "            os.makedirs(self.test_store_path)\n",
    "        vocab_dict = self.create_vocab_dict()\n",
    "        self.test_dir = os.path.join(self.chinese_root,'TEST')\n",
    "        WAV_files = sorted(glob.glob(self.test_dir+'/*.WAV'))\n",
    "        for audio_filepath in WAV_files:\n",
    "            txt_file = audio_filepath[:-4]+'.txt'\n",
    "            char_mapped,chars_list = self.create_char_mapping(txt_file,vocab_dict)\n",
    "            json_write_filepath =self.test_store_path+'/'+audio_filepath.split('\\\\')[-1][:-4]+'.json'\n",
    "            data_frame = {}\n",
    "            data_frame['audio_filepath'] = audio_filepath.replace('\\\\','/').replace('..','.')\n",
    "            data_frame['char_map_seq'] = ' '.join([str(char_item) for char_item in char_mapped])\n",
    "            data_frame['chars'] = ' '.join([str(char_item) for char_item in chars_list])\n",
    "            #print(chars_list)\n",
    "            data_frame['char_seq_len']=len(char_mapped)\n",
    "            with open(json_write_filepath, 'w',encoding='utf-8') as fid:\n",
    "                json.dump(data_frame, fid,ensure_ascii=False,indent=4)\n",
    "        \n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\"Configuration for data preparation\")\n",
    "parser.add_argument(\"--chinese_dataset_root\", default=\"../CHINESE\", type=str,help='Dataset path')\n",
    "parser.add_argument(\"--chinese_save_root\", default=\"../CHINESE/processed_data\", type=str,help='Save directory after processing')\n",
    "parser.add_argument(\"--vocab_path\",default='../data_utils/vocab.txt',type=str, help='Filepath to write vocabulary')\n",
    "\n",
    "print('start')\n",
    "config = parser.parse_known_args()[0]#parser.parse_args()\n",
    "chinese = CHINESE(config)\n",
    "\n",
    "chinese.create_vocab() #创建语料表\n",
    "chinese.process_data_train()\n",
    "chinese.process_data_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "train_files = glob.glob(\"../CHINESE/processed_data/TRAIN\"+'/*.json')\n",
    "test_files = glob.glob(\"../CHINESE/processed_data/TEST\"+'/*.json')\n",
    "\n",
    "write_train_list = open(\"../data_utils/training.txt\",'w',encoding='utf-8')\n",
    "for train_file in train_files:\n",
    "    train_file = './'+'/'.join(train_file.split('/')[1:])\n",
    "    write_train_list.write(train_file+'\\n')\n",
    "write_train_list.close()\n",
    "\n",
    "write_test_list = open(\"../data_utils/testing.txt\",'w',encoding='utf-8')\n",
    "for test_file in test_files:\n",
    "    test_file = './'+'/'.join(test_file.split('/')[1:])\n",
    "    write_test_list.write(test_file+'\\n')\n",
    "write_test_list.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
